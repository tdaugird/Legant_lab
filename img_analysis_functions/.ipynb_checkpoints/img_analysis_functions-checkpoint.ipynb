{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is series of image anlysis functions that I have been pulling together over the last little while in no particular order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in multidimensional .nd2 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''read_multidim_nd2 : last updated 06-15-20\n",
    "This is function to read in a .nd2 file that is multideminsional containing multiple fields of view. The images can be either zstacks\n",
    "or single slices. Right now this assumes that the .nd2 contains multiple fieds of view and multiple channels. There is the beginnings of \n",
    "a more robust version of this that can handle .nd2 files with different data structures. Heads up this takes a little bit of time to \n",
    "run as it reads the entire file into RAM\n",
    "\n",
    "parameters\n",
    "-------------\n",
    "filename : .nd2 file\n",
    "    this can be single or multiple fields of view, single or multiple channel, single or multiple z-slice\n",
    "final_dtype: string datatype\n",
    "    the data type of the final numpy array. This defaults to 16-bi integer\n",
    "\n",
    "Returns\n",
    "------------------------\n",
    "array_multidim: ndarray\n",
    "    this is multidimensional array. The order of the dimensions will depend on structure of .nd2 image but if defaults are all left the \n",
    "    same the order will be fov, ch, z, y, x'''\n",
    "    \n",
    "from nd2reader import ND2Reader\n",
    "import numpy as np\n",
    "    \n",
    "def read_multidim_nd2(filename, final_dtype = 'uint16'):\n",
    "    nd2_multidim = ND2Reader(filename) # read in a vdr-gr-cebpb image that has no dapi\n",
    "    if 'z' in nd2_multidim.sizes:\n",
    "        nd2_multidim.bundle_axes = 'czyx' #bundle the channel and zyx coordinates\n",
    "        nd2_multidim.iter_axes = 'v' # set the index to the field of view\n",
    "    else:\n",
    "        nd2_multidim.bundle_axes = 'cyx' #bundle the channel and zyx coordinates\n",
    "        nd2_multidim.iter_axes = 'v' # set the index to the field of view\n",
    "    array_multidim = np.array(nd2_multidim, dtype = final_dtype) #change the pims image into a numpy array containing float\n",
    "    return(array_multidim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################################################################\n",
    "#Work in progress function to convert .nd2 files into multideimsional nparray. The skeleton is here already but it is just going to \n",
    "#take either some chop wood carry water kind of scripting or some other clever way determine the structure of the input data\n",
    "###############################################################################################################################33\n",
    "\n",
    "'''This is function to read in a .nd2 file that is multideminsional containing multiple fields of view and/or multiple channels and/or\n",
    "multiple z-slices. I haven't taken the time to make to amend for time series as it is complicated to deal with the hierarchy of dimensions\n",
    "given many different data structurs but but with a little help it it should be largely convertable to make it handle this as well. It \n",
    "takes a second to work as it reads the file into ram. It reads in a .nd2 file and returns a multidemensional numpy array.\n",
    "\n",
    "parameters\n",
    "-------------\n",
    "filename : .nd2 file\n",
    "    this can be single or multiple fields of view, single or multiple channel, single or multiple z-slices\n",
    "final_dtype: string datatype\n",
    "    the data type of the final numpy array. This defaults to 16-bi integer\n",
    "\n",
    "Returns\n",
    "------------------------\n",
    "array_multidim: ndarray\n",
    "    this is multidimensional array. The order of the dimensions will depend on structure of .nd2 image but if defaults are all left the \n",
    "    same the order will be fov, ch, z, y, x\n",
    "'''\n",
    "\n",
    "from nd2reader import ND2Reader\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "def read_in_nd2(filename, final_dtype = 'uint16'):\n",
    "    nd2_multidim = ND2Reader(filename) # read in a vdr-gr-cebpb image that has no dapi\n",
    "    if 'v' not in nd2_multidim.sizes: #this would be image of a single field of view       \n",
    "        if 'z' not in nd2_multidim.sizes: # this would be a multichannel images of single focal plances\n",
    "            nd2_multidim.bundle_axes = 'yx' # bundle yx coordinates\n",
    "            nd2_multidim.iter_axes = 'c' # set the index to the channel\n",
    "        if 'c' not in nd2_multidim.sizes: #this would be single channle z-stack\n",
    "            nd2_multidim.bundle_axes = 'yx' # yx coordinates\n",
    "            nd2_multidim.iter_axes = 'z' # set the index to the z\n",
    "        else: # this would be single field of view, multichannel z-stack\n",
    "            nd2_multidim.bundle_axes = 'zyx' # zyx coordinates\n",
    "            nd2_multidim.iter_axes = 'c' # set the index to the channel\n",
    "    if 'c' not in nd2_multidim.sizes: #right now this assumes that image single channel, multiple fovs and \n",
    "        nd2_multidim.bundle_axes = 'zyx' # channel and yx coordinates\n",
    "        nd2_multidim.iter_axes = 'v' # set the index to the channel\n",
    "    if 'z' not in nd2_multidim.sizes:\n",
    "        nd2_multidim.bundle_axes = 'cyx' # channel and yx coordinates\n",
    "        nd2_multidim.iter_axes = 'v' # set the index to the channel\n",
    "    else:\n",
    "        nd2_multidim.bundle_axes = 'czyx' #bundle the channel and zyx coordinates\n",
    "        nd2_multidim.iter_axes = 'v' # set the index to the field of view\n",
    "    array_5d = np.array(nd2_multidim, dtype = final_dtype) #change the pims image into a numpy array containing float"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make z-stack from folder of .tif files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''make_z_stack: updated :06-15-20\n",
    "\n",
    "This is function for making a multihannel z-stack from from a folder containing sequential .tif files.  \n",
    "This is for making z-stacks from a folder that contains individual tiffs for each channel and z-slice. It also assumes that the naming\n",
    "conventions that are spit out of elements. i.e. z01c1, z01c2.... z##c#.  It will work on up 5 channels \n",
    "\n",
    "Parameters\n",
    "--------------\n",
    "tif_directory: string\n",
    "    This is the directory that contains the .tif files. This should be absent of other files and in the proper order. The default\n",
    "    value for this is the current working directory in order to accomodate some previous scripts in which I moved to the .tif \n",
    "    directory in the script itself\n",
    "final_dtype: string\n",
    "    this is the data type for the final output nparray\n",
    "\n",
    "Returns\n",
    "--------------\n",
    "img_arr: nparray\n",
    "    This is a numpy array that is the final z-stack all put into one array\n",
    "\n",
    "'''\n",
    "import os\n",
    "import numpy as np\n",
    "from skimage import io\n",
    "\n",
    "def make_z_stack(tif_directory = os.getcwd(), final_dtype = 'uint16'):\n",
    "    orig_dir = os.getcwd() #keep traick of working directoyr prior to moving to directory with .tif files\n",
    "    os.chdir(tif_directory) #change to directory with .tif files \n",
    "    \n",
    "    if 'Thumbs.db' in os.listdir(): #windows likes to throw these stupid files seemingly randomnly into folders during iconf view of the foler\n",
    "        os.remve('Thumbs.db') #they are basically useless to have around so it is fine to just get rid of it\n",
    "        \n",
    "    os.listdir().sort() #this is probably over kill but this code is a little fragile WRT to the order of the files\n",
    "    \n",
    "    '''determine the number of channles that this zstack hase. '''\n",
    "    channel_lst = [] # list to be populated with the channel numbers of the first six files in the folder\n",
    "    for filename in os.listdir()[:6]: #iterate over the first 6 files. Shouldn't ever be a need for more than five channels\n",
    "        channel_lst.append(int(filename[-5])) # append the list with the channel number derived from the filename\n",
    "    num_channels = max(channel_lst)\n",
    "    ###generate list of sorted images\n",
    "    image_lst = np.sort(os.listdir())\n",
    "    ### figure out shape of empty zeros array for image\n",
    "    img_ = io.imread(image_lst[0])\n",
    "    n_row = img_.shape[0]\n",
    "    n_col = img_.shape[1]\n",
    "    n_plane = int(len(image_lst) / num_channels)\n",
    "    #shape_tupple = np.zeros((n_z, n_row, n_col))\n",
    "    \n",
    "    ### generate some empyt arrays to be filled up\n",
    "    c1_img = np.zeros((n_plane, n_row, n_col))\n",
    "    c2_img = np.zeros((n_plane, n_row, n_col))\n",
    "    c3_img = np.zeros((n_plane, n_row, n_col))\n",
    "    c4_img = np.zeros((n_plane, n_row, n_col))\n",
    "    c5_img = np.zeros((n_plane, n_row, n_col))\n",
    "    \n",
    "    ###read in and generate z-stack for each channel\n",
    "    z_slice = 0\n",
    "    for i in range(0 , (len(image_lst))- num_channels, num_channels):\n",
    "        if num_channels == 1:\n",
    "            c1_img[z_slice, :, :] = io.imread(image_lst[i])\n",
    "        if num_channels == 2:\n",
    "            c1_img[z_slice, :, :] = io.imread(image_lst[i])\n",
    "            c2_img[z_slice, :, :] = io.imread(image_lst[i + 1])\n",
    "        if num_channels == 3:\n",
    "            c1_img[z_slice, :, :] = io.imread(image_lst[i])\n",
    "            c2_img[z_slice, :, :] = io.imread(image_lst[i + 1])\n",
    "            c3_img[z_slice, :, :] = io.imread(image_lst[i + 2])\n",
    "        if num_channels == 4:\n",
    "            c1_img[z_slice, :, :] = io.imread(image_lst[i])\n",
    "            c2_img[z_slice, :, :] = io.imread(image_lst[i + 1])\n",
    "            c3_img[z_slice, :, :] = io.imread(image_lst[i + 2])\n",
    "            c4_img[z_slice, :, :] = io.imread(image_lst[i + 3])\n",
    "        if num_channels == 5:\n",
    "            c1_img[z_slice, :, :] = io.imread(image_lst[i])\n",
    "            c2_img[z_slice, :, :] = io.imread(image_lst[i + 1])\n",
    "            c3_img[z_slice, :, :] = io.imread(image_lst[i + 2])\n",
    "            c4_img[z_slice, :, :] = io.imread(image_lst[i + 3])\n",
    "            c5_img[z_slice, :, :] = io.imread(image_lst[i + 4])\n",
    "        z_slice += 1\n",
    "    os.chdir(orig_dir)        \n",
    "    if num_channels == 1:\n",
    "        img_arr = np.copy(c2_img)\n",
    "        return(img_arr)\n",
    "    if num_channels == 2:\n",
    "        img_arr = np.array([c1_img, c2_img], dtype = final_dtype)\n",
    "        return(img_arr)\n",
    "    if num_channels == 3:\n",
    "        img_arr = np.array([c1_img, c2_img, c3_img], dtype = final_dtype)\n",
    "        return(img_arr)\n",
    "    if num_channels == 4:\n",
    "        img_arr = np.array([c1_img, c2_img, c3_img, c4_img], dtype = final_dtype)\n",
    "        return(img_arr)\n",
    "    if num_channels  == 5:\n",
    "        img_arr = np.array([c1_img, c2_img, c3_img, c4_img, c5_img], dtype = final_dtype)\n",
    "        return(img_arr)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## move .tif files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' move_tifs updated 06-20-20\n",
    "\n",
    "This is script to organize directory full of .tif files that is created from aquiring multichannel z-stacks at multiple fields\n",
    "of view in elements. This assumes that the nomenclature of the files if the left default in elemens. This is the same \n",
    "nomenclature that elements defaults to when writing images directly to .tif files. Basicall naming is is as follows:\n",
    "'xy'+FOV+'z'+zslice+'c'+channel+'.tif'. This organizes all of these files so that each field of view is placed in its own sub-\n",
    "directroy.\n",
    "\n",
    "Parameters\n",
    "-----------\n",
    "tif_dir: string\n",
    "    this should be in the form r'drive\\folder\\folder\\folder_containing_all_tifs'. \n",
    "    \n",
    "Returns\n",
    "-----------\n",
    "None\n",
    "'''\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def move_tifs(tif_dir):\n",
    "    \n",
    "    os.chdir(tif_dir) #change to directory with .tif files\n",
    "\n",
    "    new_dir_names = [] #temporary list to be appended with all of the first four character .tif filenames from the directory\n",
    "    for filename in os.listdir(): #iterate over all files in directory\n",
    "        if filename.endswith('.tif'): #check if til is .tif\n",
    "            new_dir_names.append(filename[:4]) #append first four characters of .tif filename to list\n",
    "    new_dir_names = list(np.unique(new_dir_names))  #make a list of only the unique entries in the list. These will be come the new subfolders for oraniziton\n",
    "\n",
    "    for dir_name in new_dir_names: #iterate the names of new subfolders and create sub folders\n",
    "        os.mkdir(dir_name)\n",
    "\n",
    "    for i in new_dir_names: # this iterates over all of the files in directory and matches individual file with appropriate newly created sub directory\n",
    "        destination_dir = tif_dir + '\\\\' + i #this is newly created sub-directory\n",
    "        tiff_files = [] # this is list to be appended with all of the .tif files in directly. This differs from above as this contains full filenames. This is technically redundent as I could move it above and save a little bit of time but I am just feeling lazy right now\n",
    "        for j in os.listdir(): # iterate over \n",
    "            if (j.endswith('.tif')): #check if tile ends\n",
    "                tiff_files.append(j) #append it to the list\n",
    "        for j in tiff_files: #iterated over all of the .tif files in the tiff_files list\n",
    "            if (j[:4] in i): #this matches the .tif file with the appropriate sub directory\n",
    "                shutil.move(tif_dir + '\\\\' + j, destination_dir + '\\\\' + j) #moves .tif file to appropriate subdirectory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purge directory of empty .tiff files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Sometimes in going through segmentation I come across image that is not worth saving so I just keep the segmentaiton blank. Becausse\n",
    "this is part of a pipeline ---see sub_cell_segmentation.ipynb ---- it is easie to just save the blank file than stop the script and \n",
    "deal with it. For this reason I wrote quick script to iterate over segmentaiton files and delete any .tif files that are blank\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "segm_dir: string - directory\n",
    "    This is the directory that contains the segmented images. It needs to be in the form:\n",
    "        r'home\\folder\\folder\\folder'\n",
    "Returns\n",
    "----------\n",
    "out_lst: list\n",
    "    This is a list of all of the blank .tif files. Files can be removed pretty easily with os.remove() function but I am just \n",
    "    paranoid and like to double check things before deleting them\n",
    "'''\n",
    "\n",
    "import os\n",
    "from skimage import io\n",
    "import numpy as np\n",
    "\n",
    "def purge_directory(segm_dir):\n",
    "    \n",
    "    current_dir = os.getcwd() # record current directoyr for coming back to at end of the function\n",
    "    os.chdir(cebp_segm_dir) #change to directory with .tif\n",
    "    \n",
    "    tiff_lst = [] #empty list to be populated with all .tif files in directory\n",
    "    for i in os.listdir(): #iterate over all of the files in the directory and add the .tif files to list\n",
    "        if 'tif' in i:\n",
    "            tiff_lst.append(i)\n",
    "\n",
    "        \n",
    "    out_lst = [] #list to tbe populated with filenames for all blank .tif files\n",
    "    for tif in tiff_lst: # iterate over all previously id'd .tif files\n",
    "        img_ = io.imread(tif) #tead the .tif \n",
    "        if np.sum(img_) == 0: # sum all pixels in image and determine if blank\n",
    "        #os.remove(i)\n",
    "            out_lst.append(tif) #add the blank .tif files to running list \n",
    "    os.chdir(current_dir) # change working directory back to where you were at beginning of functions\n",
    "    return(out_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DCTS: shannon entropy of discreate cosine transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''dcts updated 06-19-20\n",
    "\n",
    "This will calculate and return the shannon entropy of the normalized discrete cosine transform. This has proven to be a good method\n",
    "for finding the most in focus slice in a z-stack. This is taken taken from strategy used in doi: 10.1038/nbt.3708. \n",
    "\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "img_ : nparray\n",
    "    This is an image array on which to perform function\n",
    "\n",
    "Returns\n",
    "----------\n",
    "dcts_img: float\n",
    "    This is a single float value for the shannon entorpy of the discrete cosine transform of the image\n",
    "'''\n",
    "\n",
    "def dcts(img_):\n",
    "    dis_cos = dct(img_) #take the discrete cosine transform of the image\n",
    "    l2 = np.sqrt(np.sum(np.square(img_))) #perform l2 normalization\n",
    "    inner_term = np.divide(dis_cos, l2) # normalize discrete cosine transform\n",
    "    inner_term[inner_term == 0] = .0001 #it is possible that some values come outas zero. It is okay to just add a small amount to this in order to make the math work\n",
    "    first_term = np.abs(inner_term) #first term in shannon entropy absolute value of the normalize DCT\n",
    "    second_term = np.log2(np.abs(inner_term)) #second term of shannon entropy \n",
    "    dcts_img = np.multiply(-1, np.sum(np.multiply(first_term, second_term))) # this is final float value for entropy of DCTS\n",
    "    return(dcts_img)\n",
    "    #return(second_term)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dot detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''This is function to determine the xy coordinates for dot in image. This has proven handy for antibody staining stuff and \n",
    "determining the distance of puncta relative to CEBPb hubs.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "dotty_img: nparray\n",
    "    This is the original \n",
    "norm_p: float between 0 and 1\n",
    "    This is the percentile by which to normalize the image. Default divides image by max intensity\n",
    "\n",
    "The rest of the input parameters pertain to the blob_log function from skimage.feature. I have only included a subset of the \n",
    "parameters which I found are most likely to be tweeked. It can be further tuned if need be. Further documentation can be found here:\n",
    "https://scikit-image.org/docs/dev/api/skimage.feature.html#skimage.feature.blob_log\n",
    "\n",
    "min_sigmas: calar or sequence of scalars, optional\n",
    "    the minimum standard deviation for Gaussian kernel. Keep this low to detect smaller blobs. The standard deviations of the\n",
    "    Gaussian filter are given for each axis as a sequence, or as a single number, in which case it is equal for all axes.\n",
    "max_sigma: scalar or sequence of scalars, optional\n",
    "    The maximum standard deviation for Gaussian kernel. Keep this high to detect larger blobs. The standard deviations of the\n",
    "    Gaussian filter are given for each axis as a sequence, or as a single number, in which case it is equal for all axes.\n",
    "num_sigma: int, optional\n",
    "    The number of intermediate values of standard deviations to consider between min_sigma and max_sigma.\n",
    "threshold: float, optional.\n",
    "The absolute lower bound for scale space maxima. Local maxima smaller than thresh are ignored. Reduce this to detect blobs with less intensities.\n",
    "\n",
    "Returns:\n",
    "----------\n",
    "dots_y_coord: nparray\n",
    "    array of y coordinates of the dots\n",
    "dots_x_coord: nparray\n",
    "    array of x coordinates of the dots\n",
    "\n",
    "\n",
    "notes:\n",
    "also of use for tuning parameters are the follow plots\n",
    "plt.imshow(dotty_img_testing)\n",
    "plt.scatter(dots_x_coord, dots_y_coord, s=80, facecolors='none', edgecolors='r')\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from skimage.feature import blob_log\n",
    "\n",
    "def ab_dot_detecter(dotty_img,  norm_p = 1, log_sigma_min = 1, log_sigma_max = 20, log_sigma_num = 5, log_thresh = .2, log_overlap = .5):\n",
    "    normed_img = dotty_img /  np.quantile(dotty_img, norm_p)\n",
    "    dots = blob_log(normed_img, max_sigma= log_sigma_max, min_sigma = 1 num_sigma= log_sigma_num, threshold= log_thresh, \n",
    "                   overlap = log_overlap)\n",
    "    dots_y_coord = dots[:, 0]\n",
    "    dots_x_coord = dots[:, 1]\n",
    "    \n",
    "    return(dots_y_coord, dots_x_coord)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmentation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dot_2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''dot_2d last updated :06-19-20\n",
    "this is the orignal function taken from Allen institue cell segmenter:\n",
    "https://github.com/AllenInstitute/aics-segmentation/blob/master/aicssegmentation/core/seg_dot.py\n",
    "I have changed a couple of things but the basic function remains the same. This will take in an image in which hubs or dots need\n",
    "to be segmented and return a boolean array\n",
    "\n",
    "Paramters\n",
    "----------\n",
    "struct_img: nparray\n",
    "    This is the original 2D image. It does not need to be normalized as above\n",
    "log_sigma: float\n",
    "    This is the sigma of the gaussian that is used in the LOG filter. Typical values for this range from ~1-3 but it depends on \n",
    "    the size of the objects you wish to segment\n",
    "log_thresh: float\n",
    "    This is the threshold value for after the LOG filter. typical values for this range from .005 - .015 but as above this depends \n",
    "    on the size and shape of the objects you wish to segment\n",
    "norm_p: float between 0 and 1\n",
    "    This is the percentile of the intensity value by which to normalize the input image. The default normalizes the image by the maximum\n",
    "    intensity value but this may not always be the best choice. \n",
    "\n",
    "Returns\n",
    "-----------\n",
    "Returns\n",
    "----------\n",
    "bw: nparray, dtype == bool\n",
    "    This is a segmented image of the blobs\n",
    "'''\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from scipy import ndimage as ndi\n",
    "\n",
    "def dot_2d(struct_img, log_sigma = 1.5, log_thresh = 0.003, norm_p = 1): \n",
    "    struct_img_normed = struct_img / np.quantile(struct_img, norm_p) #normalize the function\n",
    "    bw = np.zeros(struct_img_normed.shape, dtype=bool) #set up an image array to become the mask\n",
    "    responce = np.zeros_like(struct_img_normed) # set up image array to handle the output of the LOG filter\n",
    "    responce = -1 * (log_sigma ** 2) * ndi.filters.gaussian_laplace(struct_img_normed, log_sigma) #This is essentially the LOG multiplied by some constants to bring into range with threhold\n",
    "    bw = responce > log_thresh # Determine which values are higher than the threshold\n",
    "    return(bw_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cebp_blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''This is script to make mask from the CEBPb blobs during adipogenesis. This requires do_2d function as a helper function. \n",
    "\n",
    "Parameters\n",
    "----------\n",
    "img: nparray\n",
    "    This is the original 2d CEBPb image\n",
    "gaussian_sigma: Float\n",
    "    This is sigma value for gaussian kernel applied to image\n",
    "log_sigma: float\n",
    "    This is the sigma of the gaussian that is used in the LOG filter. Typical values for this range from ~1-3 but it depends on \n",
    "    the size of the objects you wish to segment\n",
    "log_thresh: float\n",
    "    This is the threshold value for after the LOG filter. typical values for this range from .005 - .015 but as above this depends \n",
    "    on the size and shape of the objects you wish to segment\n",
    "norm_p: float between 0 and 1\n",
    "    This is the percentile of the intensity value by which to normalize the input image. The default normalizes the image by the maximum\n",
    "    intensity value but this may not always be the best choice. \n",
    "\n",
    "Returns\n",
    "----------\n",
    "bw: nparray\n",
    "    This is boolean array mask of the CEBPb blobs\n",
    "    \n",
    "Notes\n",
    "----------\n",
    "-The the log_sigma and log_thresh parameters have to pretty much be tuned for each cell which is a bit of a drag.\n",
    "-This also works well for DAPI heterochromatin segmentation. The log_sigma and log_thresh need to be tuned for this as well. I have found\n",
    "that starting around 1.5 * the values used for the CEBPb blob maks is a good starting place. \n",
    "'''\n",
    "from skimage.filters import gaussian\n",
    "from skimage.morphology import remove_small_objects\n",
    "\n",
    "def cebp_blobs( img, gaussian_sigma =1, log_sigma = 1.5, log_thresh = 0.003, norm_p = 1):\n",
    "    img_blurred = gaussian(img, gaussian_sigma)\n",
    "    dot_2d_out = dot_2d(img_blurred, log_sigma, log_thresh, norm_p)\n",
    "    bw = remove_small_objects(dot_2d_out)\n",
    "    return(bw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dot_2d_orig "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''dot_2d_orig\n",
    "this is the orignal function taken from Allen institue cell segmenter:\n",
    "https://github.com/AllenInstitute/aics-segmentation/blob/master/aicssegmentation/core/seg_dot.py\n",
    "It is used to detect 2d hubs within the cell. The only thing that I changed was the name of the function. The original function as\n",
    "it appear from AIC is dot_2d. \n",
    "\n",
    "Parameters\n",
    "----------\n",
    "struct_img: nparray\n",
    "    This is the normalized 2d input image in which to find the blobs\n",
    "s2_param: list of lists of two floats\n",
    "    This is a list of lists of parmeters. The first element in each list is the sigma value for the LOG. The second value for each list\n",
    "    is a threshold value.\n",
    "\n",
    "Returns\n",
    "----------\n",
    "bw: nparray, dtype == bool\n",
    "    This is a segmented image of the blobs\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "from scipy import ndimage as ndi\n",
    "\n",
    "def dot_2d_orig(struct_img, s2_param):\n",
    "    bw = np.zeros(struct_img.shape, dtype=bool)\n",
    "    for fid in range(len(s2_param)):\n",
    "        log_sigma = s2_param[fid][0]\n",
    "        responce = np.zeros_like(struct_img)\n",
    "        responce = -1*(log_sigma**2)*ndi.filters.gaussian_laplace(struct_img, log_sigma)\n",
    "        bw = np.logical_or(bw, responce>s2_param[fid][1])\n",
    "    return bw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cebp_blobs_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''This is old version of script to identify CEBPb blobs in cells during adipogenesis. It needs dot_2d_orig as a helper function\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "img_: nparray\n",
    "    This is the orignal image. No normalization neccesary\n",
    "gaussian_sigma: float\n",
    "    This is the sigma value for a gaussian filter that is used prior to finding blobs\n",
    "s2_param: list of lists of two floats\n",
    "    This is a list of lists of parmeters. The first element in each list is the sigma value for the LOG. The second value for each list\n",
    "    is a threshold value.\n",
    "\n",
    "Returns\n",
    "----------\n",
    "jnk: nparray\n",
    "    This is boolean array that is segmented image. \n",
    "'''\n",
    "\n",
    "def cebp_blobs_old(img_, gaussian_sigma = 1, s2_params = [[1.0, .03]]):\n",
    "    ################################\n",
    "    ## PARAMETERS for this step ##\n",
    "    gaussian_smoothing_sigma = gaussian_sigma\n",
    "    ################################\n",
    "    # intensity normalization\n",
    "    struct_img = np.divide(img_, np.max(img_))\n",
    "    \n",
    "    # smoothing with gaussian filter\n",
    "    structure_img_smooth = gaussian(struct_img, sigma=gaussian_smoothing_sigma)\n",
    "    \n",
    "\n",
    "    \n",
    "    ################################\n",
    "    ## PARAMETERS for this step ##\n",
    "    s2_param = s2_params\n",
    "    ################################\n",
    "    \n",
    "    bw = dot_2d_orig(structure_img_smooth, s2_param)\n",
    "        #remove small objects\n",
    "    jnk = remove_small_objects(bw)\n",
    "    \n",
    "    return(jnk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invert mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''This is function to invert a binary mask. A lot of times it is just easier to deal with the mask images if they are in 16bit rather \n",
    "than a boolean array. For example, multiplying boolean array by 16bit fluorescent image doesnt compute. This should largely take\n",
    "care of this \n",
    "\n",
    "Parameters\n",
    "----------\n",
    "orig_masked_img: nparray\n",
    "    This is the original masked image. \n",
    "Returns\n",
    "----------\n",
    "intverted_mask\n",
    "    This is the negative of the orignal mask\n",
    "'''\n",
    "\n",
    "import numpy as np \n",
    "\n",
    "def invert_mask(orig_masked_img):\n",
    "    \n",
    "    '''just to make sure that everything is in the right form I am running quick bit to guarantee it is 1s and 0s'''\n",
    "    orig_masked_img = orig_masked_img.astype('uint16') #change dtype over to 16bit\n",
    "    masked_img_max = np.max(orig_masked_img)  #determine the maximum pixel value\n",
    "    masked_img_binary = np.divide(orig_masked_img, masked_img_max) #changes to 1s and 0s\n",
    "    \n",
    "    \n",
    "    ones_bool = np.ones([masked_img_.shape[0], masked_img_.shape[1]]) #make a np array of ones the same shape as original mask image\n",
    "    inverted_mask = np.subtract(ones_bool, masked_img_binary) #subtract orignal mask from nparray of ones\n",
    "    return(inverted_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## random_blob_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-26-ed5dbd2a0aa0>, line 20)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-26-ed5dbd2a0aa0>\"\u001b[1;36m, line \u001b[1;32m20\u001b[0m\n\u001b[1;33m    def random_blob_image(nucleus_segm_img, blob_segm_img nucleus_erosion = 15)\u001b[0m\n\u001b[1;37m                                                                        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "'''updated 06-20-20\n",
    "This is for creating a binary mask in which each of the segmented sub cellular objects are rotated and move around the nucleus. In \n",
    "addition to usin segmented heterochromatin regions in dapi channel, this can serve as a decent control for determing the loaction \n",
    "of puncta etc relative to hubs\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "nucleus_segm_img: nparray\n",
    "    This is a binary mask of a single nucleus\n",
    "blob_segm_img: np array\n",
    "    This is a binary mask of the segmented sub cellular hubs\n",
    "nucleus erosion\n",
    "    This is the minimum distance from the outer edge of the nucelus that the center of each of the randomly moved blobs can be located\n",
    "Returns\n",
    "----------\n",
    "new_blob_img: nparray\n",
    "    This is a binary image in which all of the subcellular objects have been rotated and moved to random locations within the nucleus.\n",
    "\n",
    "note\n",
    "    This is note perfect. IT does not account for rotated and randomly moved subcellular objects over lapping with one anotehr. In this\n",
    "    way the total area and perimeter of the randomly moved sub cellular hubs is often less than that of the original segmented image.\n",
    "    There a way in which I could fix this but I don't feel like making perfect the enemy of good enough.\n",
    "'''\n",
    "def random_blob_image(nucleus_segm_img, blob_segm_img nucleus_erosion = 15): \n",
    "    nucleus_erode = binary_erosion(nucleus_segm_img, disk(nucleus_erosion)) #reode the segmented nucleus image so won't chooose points too close to the edge a little later on\n",
    "    \n",
    "    labeled_blob_segm_img = measure.label(blob_segm_img) #label the blobs within the image \n",
    "    \n",
    "    new_blob_img = np.zeros([nucleus_segm_img.shape[0], nucleus_segm_img.shape[1]]).astype('uint16') #set up array of zeros with same shape as original image\n",
    "    \n",
    "    for labeled_blob in np.unique(labeled_blob_segm_img)[1:]: #iterate over the blobs\n",
    "        indy_blob = np.array(labeled_blob_segm_img == labeled_blob).astype('uint8') #choose an individual blob to look at\n",
    "        minr_b, minc_b, maxr_b, maxc_b = measure.regionprops(indy_blob)[0].bbox # find coordinates of a bounding box around the individual blob\n",
    "        angle = int(np.random.randint(0, 360, 1)) #choose a random angle to ratate the blob\n",
    "        indy_blob_rotate = rotate(indy_blob[minr_b:maxr_b, minc_b: maxc_b], angle) # rotate the blob\n",
    "        thresh = threshold_otsu(indy_blob_rotate) #a rotation of a binary image does not return a binary image. Using otsu_threshold will turn the rotated image backingto a bianry image\n",
    "        indy_blob_rotate = np.array(indy_blob_rotate > thresh).astype('uint8') # turn rotated blob back into binary image\n",
    "        y_center_blob, x_center_blob = measure.regionprops(indy_blob_rotate)[0].centroid # find the center of the rotated blob\n",
    "        x_center_blob = int(x_center_blob)\n",
    "        y_center_blob = int(y_center_blob)\n",
    "        nucleus_coordinate = np.argwhere(nucleus_erode == 1) # choose random new coordinates within the nucleus\n",
    "        rand_coord_numb = int(np.random.randint(0, len(nucleus_coordinate), 1))\n",
    "        new_y_center, new_x_center = nucleus_coordinate[rand_coord_numb] # record the x and y coordinates of the randomly chosen new center\n",
    "        translate_y = int(new_y_center) - y_center_blob #amount to translate rotated blob\n",
    "        translate_x = int(new_x_center) - x_center_blob #amount to translate rotated blob\n",
    "        orig_rotated_blob_coords = np.argwhere(indy_blob_rotate == 1) # generated list of original coordinates for the rotated blob\n",
    "        ycoords_rotated_orig_pos = orig_rotated_blob_coords[:, 0] #coordinates for the \n",
    "        xcoords_rotated_orig_pos = orig_rotated_blob_coords[:, 1]\n",
    "        ycoords_rotated_new_pos = ycoords_rotated_orig_pos + translate_y\n",
    "        xcoords_rotated_new_pos = xcoords_rotated_orig_pos + translate_x\n",
    "        \n",
    "        for new_y, new_x in zip(ycoords_rotated_new_pos, xcoords_rotated_new_pos):\n",
    "            new_blob_img[new_y, new_x] = 1\n",
    "        new_blob_img = np.multiply(nucleus_segm_img, new_blob_img)\n",
    "    return(new_blob_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "img_analysis",
   "language": "python",
   "name": "img_analysis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
