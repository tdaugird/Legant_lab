{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is adapet script that aims to figure out if I can use my old NB classifier on new data. This is first one that is looking at the 8kb images. I figure if anything is going to work then it will be this one\n",
    "\n",
    "### Unfortunately it looks like I wil not be able to use my old nb classifier on this new data set. So it goes\n",
    "\n",
    "### 05-22-20 This first part of this using the random forest with the 190603 annotated .csv didn't seem to produce very good results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is the first part that is not working very well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First thing that I would like to do is check to make sure random tree model is working as anticipated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=None, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''chane to the directory with the training data'''\n",
    "os.chdir(r'C:\\Users\\LegantLab\\Documents\\git\\tad\\Legant_lab\\200428_adipogenesis_timecourses_revisited\\190603_3t3l1_timecourse\\version2\\8kb_run\\annotation_csvs')\n",
    "df_train_orig = pd.read_csv('annotated_training.csv')\n",
    "keep_cats = [ 'AreaShape_Area',\n",
    " #'AreaShape_Center_X',\n",
    " #'AreaShape_Center_Y',\n",
    " #'AreaShape_Center_Z',\n",
    " 'AreaShape_Compactness',\n",
    " 'AreaShape_Eccentricity',\n",
    " 'AreaShape_EulerNumber',\n",
    " 'AreaShape_Extent',\n",
    " 'AreaShape_FormFactor',\n",
    " 'AreaShape_MajorAxisLength',\n",
    " 'AreaShape_MaxFeretDiameter',\n",
    " 'AreaShape_MaximumRadius',\n",
    " 'AreaShape_MeanRadius',\n",
    " 'AreaShape_MedianRadius',\n",
    " 'AreaShape_MinFeretDiameter',\n",
    " 'AreaShape_MinorAxisLength',\n",
    " 'AreaShape_Orientation',\n",
    " 'AreaShape_Perimeter',\n",
    " 'AreaShape_Solidity',\n",
    " 'AreaShape_Zernike_0_0',\n",
    " 'AreaShape_Zernike_1_1',\n",
    " 'AreaShape_Zernike_2_0',\n",
    " 'AreaShape_Zernike_2_2',\n",
    " 'AreaShape_Zernike_3_1',\n",
    " 'AreaShape_Zernike_3_3',\n",
    " 'AreaShape_Zernike_4_0',\n",
    " 'AreaShape_Zernike_4_2',\n",
    " 'AreaShape_Zernike_4_4',\n",
    " 'AreaShape_Zernike_5_1',\n",
    " 'AreaShape_Zernike_5_3',\n",
    " 'AreaShape_Zernike_5_5',\n",
    " 'AreaShape_Zernike_6_0',\n",
    " 'AreaShape_Zernike_6_2',\n",
    " 'AreaShape_Zernike_6_4',\n",
    " 'AreaShape_Zernike_6_6',\n",
    " 'AreaShape_Zernike_7_1',\n",
    " 'AreaShape_Zernike_7_3',\n",
    " 'AreaShape_Zernike_7_5',\n",
    " 'AreaShape_Zernike_7_7',\n",
    " 'AreaShape_Zernike_8_0',\n",
    " 'AreaShape_Zernike_8_2',\n",
    " 'AreaShape_Zernike_8_4',\n",
    " 'AreaShape_Zernike_8_6',\n",
    " 'AreaShape_Zernike_8_8',\n",
    " 'AreaShape_Zernike_9_1',\n",
    " 'AreaShape_Zernike_9_3',\n",
    " 'AreaShape_Zernike_9_5',\n",
    " 'AreaShape_Zernike_9_7',\n",
    " 'AreaShape_Zernike_9_9',\n",
    " 'Intensity_IntegratedIntensityEdge_dapi',\n",
    " 'Intensity_IntegratedIntensity_dapi',\n",
    " 'Intensity_LowerQuartileIntensity_dapi',\n",
    " 'Intensity_MADIntensity_dapi',\n",
    " 'Intensity_MassDisplacement_dapi',\n",
    " 'Intensity_MaxIntensityEdge_dapi',\n",
    " 'Intensity_MaxIntensity_dapi',\n",
    " 'Intensity_MeanIntensityEdge_dapi',\n",
    " 'Intensity_MeanIntensity_dapi',\n",
    " 'Intensity_MedianIntensity_dapi',\n",
    " 'Intensity_MinIntensityEdge_dapi',\n",
    " 'Intensity_MinIntensity_dapi',\n",
    " 'Intensity_StdIntensityEdge_dapi',\n",
    " 'Intensity_StdIntensity_dapi',\n",
    " 'Intensity_UpperQuartileIntensity_dapi',\n",
    "'in_object']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''read in the training data and drop categories and entries that are of no use'''\n",
    "df_train = df_train_orig[keep_cats] # make a copy of original df. I am going to alter this a little bit in order to pull out train and test cats\n",
    "df_train = df_train.dropna()\n",
    "\n",
    "'''this is target category for the training the classifier'''\n",
    "target_cat_name = 'in_object'\n",
    "\n",
    "'''remove unwanted categories'''\n",
    "#unwanted_cat = ['Unnamed: 0']#, 'ImageNumber', 'ObjectNumber']\n",
    "#df_train.drop(unwanted_cat, axis = 1, inplace = True)\n",
    "\n",
    "'''define train and target data'''\n",
    "x_train = df_train.drop('in_object', axis = 1)\n",
    "y_train = df_train[target_cat_name]\n",
    "\n",
    "#x_test = df_\n",
    "#y_test = df_whole.[target_cat_name]\n",
    "\n",
    "#'''split data into train and test sets'''\n",
    "#x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = .25, random_state = 42)\n",
    "\n",
    "'''this is a feature scaling step. This will standardize all of the data in order to pull everything into the same range'''\n",
    "sc_X = StandardScaler()\n",
    "x_train = sc_X.fit_transform(x_train)\n",
    "#x_test = sc_X.transform(x_test)\n",
    "\n",
    "'''random forest classifier'''\n",
    "clf=RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now I will need to read in and test annotated data from the 190798 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r'C:\\Users\\LegantLab\\Documents\\git\\tad\\Legant_lab\\200428_adipogenesis_timecourses_revisited\\190718_3t3l1_time_course_r5\\annotated_csvs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['df_in_stad3-29-xy5c1.csv',\n",
       " 'df_in_stad3-34.csv',\n",
       " 'df_out_stad3-29-xy5c1.csv',\n",
       " 'df_out_stad3-34.csv']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_in_329 = pd.read_csv('df_in_stad3-29-xy5c1.csv')\n",
    "df_in_329['in_object'] = np.ones(len(df_in_329))\n",
    "df_in_334 = pd.read_csv('df_in_stad3-34.csv')\n",
    "df_in_334['in_object'] = np.ones(len(df_in_334))\n",
    "df_out_329 = pd.read_csv('df_out_stad3-29-xy5c1.csv')\n",
    "df_out_329['in_object'] = np.zeros(len(df_out_329))\n",
    "df_out_334 = pd.read_csv('df_out_stad3-34.csv')\n",
    "df_out_334['in_object'] = np.zeros(len(df_out_334))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_whole = pd.concat([df_in_329, df_in_334, df_out_329, df_out_334])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9130434782608695"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_test_whole.loc[df_test_whole.in_object == 1]) / len(df_test_whole.loc[df_test_whole.in_object == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r'C:\\Users\\LegantLab\\Documents\\git\\tad\\Legant_lab\\200428_adipogenesis_timecourses_revisited\\190718_3t3l1_time_course_r5\\annotated_csvs')\n",
    "df_in_329 = pd.read_csv('df_in_stad3-29-xy5c1.csv')\n",
    "df_in_329['in_object'] = np.ones(len(df_in_329))\n",
    "df_in_334 = pd.read_csv('df_in_stad3-34.csv')\n",
    "df_in_334['in_object'] = np.ones(len(df_in_334))\n",
    "df_out_329 = pd.read_csv('df_out_stad3-29-xy5c1.csv')\n",
    "df_out_329['in_object'] = np.zeros(len(df_out_329))\n",
    "df_out_334 = pd.read_csv('df_out_stad3-34.csv')\n",
    "df_out_334['in_object'] = np.zeros(len(df_out_334))\n",
    "df_test_whole = pd.concat([df_in_329, df_in_334, df_out_329, df_out_334])\n",
    "df_test = df_test_whole[keep_cats]\n",
    "x_test = df_test.drop('in_object', axis = 1)\n",
    "x_test = sc_X.transform(x_test)\n",
    "y_test = df_test[target_cat_name]\n",
    "'''make predictions based on the trained data'''\n",
    "y_pred = clf.predict(x_test)\n",
    "'''double check accuracy and precisiton of model'''\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "\n",
    "print('accuracy: ', accuracy, ' precision: ', precision)\n",
    "confusion_matrix(df_test.in_object, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_test_whole[keep_cats]\n",
    "x_test = df_test.drop('in_object', axis = 1)\n",
    "x_test = sc_X.transform(x_test)\n",
    "y_test = df_test[target_cat_name]\n",
    "'''make predictions based on the trained data'''\n",
    "y_pred = clf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.5454545454545454  precision:  1.0\n"
     ]
    }
   ],
   "source": [
    "'''double check accuracy and precisiton of model'''\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "\n",
    "print('accuracy: ', accuracy, ' precision: ', precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[23,  0],\n",
       "       [20,  1]], dtype=int64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(df_test.in_object, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clearly something is not work as expected here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LegantLab\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test['in_object_predict'] = clf.predict(x_test)\n",
    "tp = len(df_test.loc[(df_test.in_object == 1) & (df_test.in_object_predict == 1)])\n",
    "tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tn = len(df_test.loc[(df_test.in_object == 0) & (df_test.in_object_predict == 1)])\n",
    "tn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0  :  0\n",
      "ImageNumber  :  0\n",
      "ObjectNumber  :  0\n",
      "AreaShape_Area  :  0\n",
      "AreaShape_Center_X  :  0\n",
      "AreaShape_Center_Y  :  0\n",
      "AreaShape_Center_Z  :  0\n",
      "AreaShape_Compactness  :  0\n",
      "AreaShape_Eccentricity  :  0\n",
      "AreaShape_EulerNumber  :  0\n",
      "AreaShape_Extent  :  0\n",
      "AreaShape_FormFactor  :  0\n",
      "AreaShape_MajorAxisLength  :  0\n",
      "AreaShape_MaxFeretDiameter  :  0\n",
      "AreaShape_MaximumRadius  :  0\n",
      "AreaShape_MeanRadius  :  0\n",
      "AreaShape_MedianRadius  :  0\n",
      "AreaShape_MinFeretDiameter  :  0\n",
      "AreaShape_MinorAxisLength  :  0\n",
      "AreaShape_Orientation  :  0\n",
      "AreaShape_Perimeter  :  0\n",
      "AreaShape_Solidity  :  0\n",
      "AreaShape_Zernike_0_0  :  0\n",
      "AreaShape_Zernike_1_1  :  0\n",
      "AreaShape_Zernike_2_0  :  0\n",
      "AreaShape_Zernike_2_2  :  0\n",
      "AreaShape_Zernike_3_1  :  0\n",
      "AreaShape_Zernike_3_3  :  0\n",
      "AreaShape_Zernike_4_0  :  0\n",
      "AreaShape_Zernike_4_2  :  0\n",
      "AreaShape_Zernike_4_4  :  0\n",
      "AreaShape_Zernike_5_1  :  0\n",
      "AreaShape_Zernike_5_3  :  0\n",
      "AreaShape_Zernike_5_5  :  0\n",
      "AreaShape_Zernike_6_0  :  0\n",
      "AreaShape_Zernike_6_2  :  0\n",
      "AreaShape_Zernike_6_4  :  0\n",
      "AreaShape_Zernike_6_6  :  0\n",
      "AreaShape_Zernike_7_1  :  0\n",
      "AreaShape_Zernike_7_3  :  0\n",
      "AreaShape_Zernike_7_5  :  0\n",
      "AreaShape_Zernike_7_7  :  0\n",
      "AreaShape_Zernike_8_0  :  0\n",
      "AreaShape_Zernike_8_2  :  0\n",
      "AreaShape_Zernike_8_4  :  0\n",
      "AreaShape_Zernike_8_6  :  0\n",
      "AreaShape_Zernike_8_8  :  0\n",
      "AreaShape_Zernike_9_1  :  0\n",
      "AreaShape_Zernike_9_3  :  0\n",
      "AreaShape_Zernike_9_5  :  0\n",
      "AreaShape_Zernike_9_7  :  0\n",
      "AreaShape_Zernike_9_9  :  0\n",
      "Intensity_IntegratedIntensityEdge_cebp  :  0\n",
      "Intensity_IntegratedIntensityEdge_dapi  :  0\n",
      "Intensity_IntegratedIntensityEdge_ppar  :  0\n",
      "Intensity_IntegratedIntensity_cebp  :  0\n",
      "Intensity_IntegratedIntensity_dapi  :  0\n",
      "Intensity_IntegratedIntensity_ppar  :  0\n",
      "Intensity_LowerQuartileIntensity_cebp  :  0\n",
      "Intensity_LowerQuartileIntensity_dapi  :  0\n",
      "Intensity_LowerQuartileIntensity_ppar  :  0\n",
      "Intensity_MADIntensity_cebp  :  0\n",
      "Intensity_MADIntensity_dapi  :  0\n",
      "Intensity_MADIntensity_ppar  :  0\n",
      "Intensity_MassDisplacement_cebp  :  0\n",
      "Intensity_MassDisplacement_dapi  :  0\n",
      "Intensity_MassDisplacement_ppar  :  0\n",
      "Intensity_MaxIntensityEdge_cebp  :  0\n",
      "Intensity_MaxIntensityEdge_dapi  :  0\n",
      "Intensity_MaxIntensityEdge_ppar  :  0\n",
      "Intensity_MaxIntensity_cebp  :  0\n",
      "Intensity_MaxIntensity_dapi  :  0\n",
      "Intensity_MaxIntensity_ppar  :  0\n",
      "Intensity_MeanIntensityEdge_cebp  :  0\n",
      "Intensity_MeanIntensityEdge_dapi  :  0\n",
      "Intensity_MeanIntensityEdge_ppar  :  0\n",
      "Intensity_MeanIntensity_cebp  :  0\n",
      "Intensity_MeanIntensity_dapi  :  0\n",
      "Intensity_MeanIntensity_ppar  :  0\n",
      "Intensity_MedianIntensity_cebp  :  0\n",
      "Intensity_MedianIntensity_dapi  :  0\n",
      "Intensity_MedianIntensity_ppar  :  0\n",
      "Intensity_MinIntensityEdge_cebp  :  0\n",
      "Intensity_MinIntensityEdge_dapi  :  0\n",
      "Intensity_MinIntensityEdge_ppar  :  0\n",
      "Intensity_MinIntensity_cebp  :  0\n",
      "Intensity_MinIntensity_dapi  :  0\n",
      "Intensity_MinIntensity_ppar  :  0\n",
      "Intensity_StdIntensityEdge_cebp  :  0\n",
      "Intensity_StdIntensityEdge_dapi  :  0\n",
      "Intensity_StdIntensityEdge_ppar  :  0\n",
      "Intensity_StdIntensity_cebp  :  0\n",
      "Intensity_StdIntensity_dapi  :  0\n",
      "Intensity_StdIntensity_ppar  :  0\n",
      "Intensity_UpperQuartileIntensity_cebp  :  0\n",
      "Intensity_UpperQuartileIntensity_dapi  :  0\n",
      "Intensity_UpperQuartileIntensity_ppar  :  0\n",
      "Location_CenterMassIntensity_X_cebp  :  0\n",
      "Location_CenterMassIntensity_X_dapi  :  0\n",
      "Location_CenterMassIntensity_X_ppar  :  0\n",
      "Location_CenterMassIntensity_Y_cebp  :  0\n",
      "Location_CenterMassIntensity_Y_dapi  :  0\n",
      "Location_CenterMassIntensity_Y_ppar  :  0\n",
      "Location_CenterMassIntensity_Z_cebp  :  0\n",
      "Location_CenterMassIntensity_Z_dapi  :  0\n",
      "Location_CenterMassIntensity_Z_ppar  :  0\n",
      "Location_Center_X  :  0\n",
      "Location_Center_Y  :  0\n",
      "Location_Center_Z  :  0\n",
      "Location_MaxIntensity_X_cebp  :  0\n",
      "Location_MaxIntensity_X_dapi  :  0\n",
      "Location_MaxIntensity_X_ppar  :  0\n",
      "Location_MaxIntensity_Y_cebp  :  0\n",
      "Location_MaxIntensity_Y_dapi  :  0\n",
      "Location_MaxIntensity_Y_ppar  :  0\n",
      "Location_MaxIntensity_Z_cebp  :  0\n",
      "Location_MaxIntensity_Z_dapi  :  0\n",
      "Location_MaxIntensity_Z_ppar  :  0\n",
      "Number_Object_Number  :  0\n",
      "in_object  :  0\n"
     ]
    }
   ],
   "source": [
    "for i in list(df_test_whole):\n",
    "    print(i, ' : ', np.sum(df_test_whole[i].isna()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_cats = [\n",
    " 'AreaShape_Area',\n",
    " 'AreaShape_Center_X',\n",
    " 'AreaShape_Center_Y',\n",
    " 'AreaShape_Center_Z',\n",
    " 'AreaShape_Compactness',\n",
    " 'AreaShape_Eccentricity',\n",
    " 'AreaShape_EulerNumber',\n",
    " 'AreaShape_Extent',\n",
    " 'AreaShape_FormFactor',\n",
    " 'AreaShape_MajorAxisLength',\n",
    " 'AreaShape_MaxFeretDiameter',\n",
    " 'AreaShape_MaximumRadius',\n",
    " 'AreaShape_MeanRadius',\n",
    " 'AreaShape_MedianRadius',\n",
    " 'AreaShape_MinFeretDiameter',\n",
    " 'AreaShape_MinorAxisLength',\n",
    " 'AreaShape_Orientation',\n",
    " 'AreaShape_Perimeter',\n",
    " 'AreaShape_Solidity',\n",
    " 'Intensity_IntegratedIntensityEdge_dapi',\n",
    " 'Intensity_IntegratedIntensity_dapi',\n",
    " 'Intensity_LowerQuartileIntensity_dapi',\n",
    " 'Intensity_MADIntensity_dapi', \n",
    " 'Intensity_MaxIntensityEdge_dapi',\n",
    " 'Intensity_MaxIntensity_dapi',\n",
    " 'Intensity_MeanIntensityEdge_dapi',\n",
    " 'Intensity_MeanIntensity_dapi',\n",
    " 'Intensity_MedianIntensity_dapi',\n",
    " 'Intensity_MinIntensityEdge_dapi',\n",
    " 'Intensity_MinIntensity_dapi',\n",
    " 'Intensity_StdIntensityEdge_dapi',\n",
    " 'Intensity_StdIntensity_dapi',\n",
    " 'Intensity_UpperQuartileIntensity_dapi',\n",
    " 'Location_CenterMassIntensity_X_dapi',\n",
    " 'Location_CenterMassIntensity_Y_dapi',\n",
    " 'Location_CenterMassIntensity_Z_dapi',\n",
    " 'Location_Center_X',\n",
    " 'Location_Center_Y',\n",
    " 'Location_Center_Z',\n",
    " 'Location_MaxIntensity_X_dapi',\n",
    " 'Location_MaxIntensity_Y_dapi',\n",
    " 'Location_MaxIntensity_Z_dapi',\n",
    " 'in_object']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None, var_smoothing=1e-09)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(r'C:\\Users\\LegantLab\\Documents\\git\\tad\\Legant_lab\\200428_adipogenesis_timecourses_revisited\\190603_3t3l1_timecourse\\8kb_run')\n",
    "\n",
    "df_whole = pd.read_csv('data_for_nb_training.csv')\n",
    "\n",
    "\n",
    "\n",
    "'''split the data into train and target sets. In this caes x will be the train data and y will be the target parameter'''\n",
    "df = df_whole[keep_cats] # make a copy of original df. I am going to alter this a little bit in order to pull out train and test cats\n",
    "\n",
    "target_cat_name = 'in_object'\n",
    "\n",
    "'''remove unwanted categories'''\n",
    "#unwanted_cat = ['Unnamed: 0']#, 'ImageNumber', 'ObjectNumber']\n",
    "#df.drop(unwanted_cat, axis = 1, inplace = True)\n",
    "\n",
    "'''define train and target data'''\n",
    "x = df.drop('in_object', axis = 1)\n",
    "y = df[target_cat_name]\n",
    "\n",
    "'''split data into train and test sets'''\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = .25, random_state = 42)\n",
    "\n",
    "'''this is a feature scaling step. This will standardize all of the data in order to pull everything into the same range'''\n",
    "\n",
    "sc_X = StandardScaler()\n",
    "x_train = sc_X.fit_transform(x_train)\n",
    "x_test = sc_X.transform(x_test)\n",
    "\n",
    "'''set up gaussian naive bayes model and fit it to the data'''\n",
    "model = GaussianNB()\n",
    "model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''make predictions based on the trained data'''\n",
    "y_pred = model.predict(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.8936170212765957  precision:  0.9122807017543859\n"
     ]
    }
   ],
   "source": [
    "'''double check accuracy and precisiton of model'''\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "\n",
    "print('accuracy: ', accuracy, ' precision: ', precision)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[23,  0],\n",
       "       [19,  2]], dtype=int64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(df_test.in_object, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The above didn't work very well so I am going to pivot and try to use the annotated data from the 200217 data set to generate a model instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''move to directory with annotated .csv files'''\n",
    "os.chdir(r'C:\\Users\\LegantLab\\Documents\\git\\tad\\Legant_lab\\200217_adipogen_zstacks\\datases1_and2\\dcts_output\\segm_img_tests\\in_out_csvs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''change to directory with annotated data from 200217'''\n",
    "os.chdir(r'C:\\Users\\LegantLab\\Documents\\git\\tad\\Legant_lab\\200217_adipogen_zstacks\\datases1_and2\\dcts_output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints',\n",
       " 'classifier.ipynb',\n",
       " 'data_for_nb_training.csv',\n",
       " 'df1_nb_parsed.csv',\n",
       " 'df2_nb_parsed.csv',\n",
       " 'img_ls.csv',\n",
       " 'MyExpt_Experiment.csv',\n",
       " 'MyExpt_Image.csv',\n",
       " 'MyExpt_nuclei.csv',\n",
       " 'nb_filter_testing',\n",
       " 'segm_exp_analysis_nb_parsed.ipynb',\n",
       " 'segm_img_tests',\n",
       " 'segm_parse_testing.ipynb']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_200217 = pd.read_csv('data_for_nb_training.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Unnamed: 0',\n",
       " 'ImageNumber',\n",
       " 'ObjectNumber',\n",
       " 'AreaShape_Area',\n",
       " 'AreaShape_Center_X',\n",
       " 'AreaShape_Center_Y',\n",
       " 'AreaShape_Center_Z',\n",
       " 'AreaShape_Compactness',\n",
       " 'AreaShape_Eccentricity',\n",
       " 'AreaShape_EulerNumber',\n",
       " 'AreaShape_Extent',\n",
       " 'AreaShape_FormFactor',\n",
       " 'AreaShape_MajorAxisLength',\n",
       " 'AreaShape_MaxFeretDiameter',\n",
       " 'AreaShape_MaximumRadius',\n",
       " 'AreaShape_MeanRadius',\n",
       " 'AreaShape_MedianRadius',\n",
       " 'AreaShape_MinFeretDiameter',\n",
       " 'AreaShape_MinorAxisLength',\n",
       " 'AreaShape_Orientation',\n",
       " 'AreaShape_Perimeter',\n",
       " 'AreaShape_Solidity',\n",
       " 'Intensity_IntegratedIntensityEdge_c2',\n",
       " 'Intensity_IntegratedIntensityEdge_c3',\n",
       " 'Intensity_IntegratedIntensityEdge_cebp',\n",
       " 'Intensity_IntegratedIntensityEdge_dapi',\n",
       " 'Intensity_IntegratedIntensity_c2',\n",
       " 'Intensity_IntegratedIntensity_c3',\n",
       " 'Intensity_IntegratedIntensity_cebp',\n",
       " 'Intensity_IntegratedIntensity_dapi',\n",
       " 'Intensity_LowerQuartileIntensity_c2',\n",
       " 'Intensity_LowerQuartileIntensity_c3',\n",
       " 'Intensity_LowerQuartileIntensity_cebp',\n",
       " 'Intensity_LowerQuartileIntensity_dapi',\n",
       " 'Intensity_MADIntensity_c2',\n",
       " 'Intensity_MADIntensity_c3',\n",
       " 'Intensity_MADIntensity_cebp',\n",
       " 'Intensity_MADIntensity_dapi',\n",
       " 'Intensity_MassDisplacement_c2',\n",
       " 'Intensity_MassDisplacement_c3',\n",
       " 'Intensity_MassDisplacement_cebp',\n",
       " 'Intensity_MassDisplacement_dapi',\n",
       " 'Intensity_MaxIntensityEdge_c2',\n",
       " 'Intensity_MaxIntensityEdge_c3',\n",
       " 'Intensity_MaxIntensityEdge_cebp',\n",
       " 'Intensity_MaxIntensityEdge_dapi',\n",
       " 'Intensity_MaxIntensity_c2',\n",
       " 'Intensity_MaxIntensity_c3',\n",
       " 'Intensity_MaxIntensity_cebp',\n",
       " 'Intensity_MaxIntensity_dapi',\n",
       " 'Intensity_MeanIntensityEdge_c2',\n",
       " 'Intensity_MeanIntensityEdge_c3',\n",
       " 'Intensity_MeanIntensityEdge_cebp',\n",
       " 'Intensity_MeanIntensityEdge_dapi',\n",
       " 'Intensity_MeanIntensity_c2',\n",
       " 'Intensity_MeanIntensity_c3',\n",
       " 'Intensity_MeanIntensity_cebp',\n",
       " 'Intensity_MeanIntensity_dapi',\n",
       " 'Intensity_MedianIntensity_c2',\n",
       " 'Intensity_MedianIntensity_c3',\n",
       " 'Intensity_MedianIntensity_cebp',\n",
       " 'Intensity_MedianIntensity_dapi',\n",
       " 'Intensity_MinIntensityEdge_c2',\n",
       " 'Intensity_MinIntensityEdge_c3',\n",
       " 'Intensity_MinIntensityEdge_cebp',\n",
       " 'Intensity_MinIntensityEdge_dapi',\n",
       " 'Intensity_MinIntensity_c2',\n",
       " 'Intensity_MinIntensity_c3',\n",
       " 'Intensity_MinIntensity_cebp',\n",
       " 'Intensity_MinIntensity_dapi',\n",
       " 'Intensity_StdIntensityEdge_c2',\n",
       " 'Intensity_StdIntensityEdge_c3',\n",
       " 'Intensity_StdIntensityEdge_cebp',\n",
       " 'Intensity_StdIntensityEdge_dapi',\n",
       " 'Intensity_StdIntensity_c2',\n",
       " 'Intensity_StdIntensity_c3',\n",
       " 'Intensity_StdIntensity_cebp',\n",
       " 'Intensity_StdIntensity_dapi',\n",
       " 'Intensity_UpperQuartileIntensity_c2',\n",
       " 'Intensity_UpperQuartileIntensity_c3',\n",
       " 'Intensity_UpperQuartileIntensity_cebp',\n",
       " 'Intensity_UpperQuartileIntensity_dapi',\n",
       " 'Location_CenterMassIntensity_X_c2',\n",
       " 'Location_CenterMassIntensity_X_c3',\n",
       " 'Location_CenterMassIntensity_X_cebp',\n",
       " 'Location_CenterMassIntensity_X_dapi',\n",
       " 'Location_CenterMassIntensity_Y_c2',\n",
       " 'Location_CenterMassIntensity_Y_c3',\n",
       " 'Location_CenterMassIntensity_Y_cebp',\n",
       " 'Location_CenterMassIntensity_Y_dapi',\n",
       " 'Location_CenterMassIntensity_Z_c2',\n",
       " 'Location_CenterMassIntensity_Z_c3',\n",
       " 'Location_CenterMassIntensity_Z_cebp',\n",
       " 'Location_CenterMassIntensity_Z_dapi',\n",
       " 'Location_Center_X',\n",
       " 'Location_Center_Y',\n",
       " 'Location_Center_Z',\n",
       " 'Location_MaxIntensity_X_c2',\n",
       " 'Location_MaxIntensity_X_c3',\n",
       " 'Location_MaxIntensity_X_cebp',\n",
       " 'Location_MaxIntensity_X_dapi',\n",
       " 'Location_MaxIntensity_Y_c2',\n",
       " 'Location_MaxIntensity_Y_c3',\n",
       " 'Location_MaxIntensity_Y_cebp',\n",
       " 'Location_MaxIntensity_Y_dapi',\n",
       " 'Location_MaxIntensity_Z_c2',\n",
       " 'Location_MaxIntensity_Z_c3',\n",
       " 'Location_MaxIntensity_Z_cebp',\n",
       " 'Location_MaxIntensity_Z_dapi',\n",
       " 'Number_Object_Number',\n",
       " 'in_object']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df_train_200217)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Okay so this dataset does not include the Zerneke numbers but I think that it could still be goof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=None, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''generate a random tree model based on the training data'''\n",
    "\n",
    "df_train_orig = df_train_200217.copy()\n",
    "\n",
    "keep_cats = [ 'AreaShape_Area',\n",
    " #'AreaShape_Center_X',\n",
    " #'AreaShape_Center_Y',\n",
    " #'AreaShape_Center_Z',\n",
    " 'AreaShape_Compactness',\n",
    " 'AreaShape_Eccentricity',\n",
    " 'AreaShape_EulerNumber',\n",
    " 'AreaShape_Extent',\n",
    " 'AreaShape_FormFactor',\n",
    " 'AreaShape_MajorAxisLength',\n",
    " 'AreaShape_MaxFeretDiameter',\n",
    " 'AreaShape_MaximumRadius',\n",
    " 'AreaShape_MeanRadius',\n",
    " 'AreaShape_MedianRadius',\n",
    " 'AreaShape_MinFeretDiameter',\n",
    " 'AreaShape_MinorAxisLength',\n",
    " 'AreaShape_Orientation',\n",
    " 'AreaShape_Perimeter',\n",
    " 'AreaShape_Solidity',\n",
    " 'Intensity_IntegratedIntensityEdge_dapi',\n",
    " 'Intensity_IntegratedIntensity_dapi',\n",
    " 'Intensity_LowerQuartileIntensity_dapi',\n",
    " 'Intensity_MADIntensity_dapi',\n",
    " 'Intensity_MassDisplacement_dapi',\n",
    " 'Intensity_MaxIntensityEdge_dapi',\n",
    " 'Intensity_MaxIntensity_dapi',\n",
    " 'Intensity_MeanIntensityEdge_dapi',\n",
    " 'Intensity_MeanIntensity_dapi',\n",
    " 'Intensity_MedianIntensity_dapi',\n",
    " 'Intensity_MinIntensityEdge_dapi',\n",
    " 'Intensity_MinIntensity_dapi',\n",
    " 'Intensity_StdIntensityEdge_dapi',\n",
    " 'Intensity_StdIntensity_dapi',\n",
    " 'Intensity_UpperQuartileIntensity_dapi',\n",
    "'in_object']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''read in the training data and drop categories and entries that are of no use'''\n",
    "df_train = df_train_orig[keep_cats] # make a copy of original df. I am going to alter this a little bit in order to pull out train and test cats\n",
    "df_train = df_train.dropna()\n",
    "\n",
    "'''this is target category for the training the classifier'''\n",
    "target_cat_name = 'in_object'\n",
    "\n",
    "'''remove unwanted categories'''\n",
    "#unwanted_cat = ['Unnamed: 0']#, 'ImageNumber', 'ObjectNumber']\n",
    "#df_train.drop(unwanted_cat, axis = 1, inplace = True)\n",
    "\n",
    "'''define train and target data'''\n",
    "x_train = df_train.drop('in_object', axis = 1)\n",
    "y_train = df_train[target_cat_name]\n",
    "\n",
    "#x_test = df_\n",
    "#y_test = df_whole.[target_cat_name]\n",
    "\n",
    "#'''split data into train and test sets'''\n",
    "#x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = .25, random_state = 42)\n",
    "\n",
    "'''this is a feature scaling step. This will standardize all of the data in order to pull everything into the same range'''\n",
    "sc_X = StandardScaler()\n",
    "x_train = sc_X.fit_transform(x_train)\n",
    "#x_test = sc_X.transform(x_test)\n",
    "\n",
    "'''random forest classifier'''\n",
    "clf=RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''read in the test data'''\n",
    "os.chdir(r'C:\\Users\\LegantLab\\Documents\\git\\tad\\Legant_lab\\200428_adipogenesis_timecourses_revisited\\190718_3t3l1_time_course_r5\\annotated_csvs')\n",
    "df_in_329 = pd.read_csv('df_in_stad3-29-xy5c1.csv')\n",
    "df_in_329['in_object'] = np.ones(len(df_in_329))\n",
    "df_in_334 = pd.read_csv('df_in_stad3-34.csv')\n",
    "df_in_334['in_object'] = np.ones(len(df_in_334))\n",
    "df_out_329 = pd.read_csv('df_out_stad3-29-xy5c1.csv')\n",
    "df_out_329['in_object'] = np.zeros(len(df_out_329))\n",
    "df_out_334 = pd.read_csv('df_out_stad3-34.csv')\n",
    "df_out_334['in_object'] = np.zeros(len(df_out_334))\n",
    "df_test_whole = pd.concat([df_in_329, df_in_334, df_out_329, df_out_334])\n",
    "\n",
    "'''pull out parameters and target parameter'''\n",
    "df_test = df_test_whole[keep_cats]\n",
    "x_test = df_test.drop('in_object', axis = 1)\n",
    "x_test = sc_X.transform(x_test)\n",
    "y_test = df_test[target_cat_name]\n",
    "\n",
    "'''make predictions based on the trained data'''\n",
    "y_pred = clf.predict(x_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.7954545454545454  precision:  0.7307692307692307\n"
     ]
    }
   ],
   "source": [
    "'''double check accuracy and precisiton of model'''\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "\n",
    "print('accuracy: ', accuracy, ' precision: ', precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[16,  7],\n",
       "       [ 2, 19]], dtype=int64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LegantLab\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test['in_object_predict'] = clf.predict(x_test)\n",
    "tp = len(df_test.loc[(df_test.in_object == 1) & (df_test.in_object_predict == 1)])\n",
    "tp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is less than ideal but I think that it is still pretty workable. I am getting a lot of false negatives but it is still doing a good job of pulling out the true positives without keeping false positives\n",
    "\n",
    "### as a quick test I am going to see how much of the data this throws out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r'C:\\Users\\LegantLab\\Documents\\git\\tad\\Legant_lab\\200428_adipogenesis_timecourses_revisited\\190718_3t3l1_time_course_r5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_200217 = pd.read_csv('MyExpt_wholenuclei.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AreaShape_Area',\n",
       " 'AreaShape_Compactness',\n",
       " 'AreaShape_Eccentricity',\n",
       " 'AreaShape_EulerNumber',\n",
       " 'AreaShape_Extent',\n",
       " 'AreaShape_FormFactor',\n",
       " 'AreaShape_MajorAxisLength',\n",
       " 'AreaShape_MaxFeretDiameter',\n",
       " 'AreaShape_MaximumRadius',\n",
       " 'AreaShape_MeanRadius',\n",
       " 'AreaShape_MedianRadius',\n",
       " 'AreaShape_MinFeretDiameter',\n",
       " 'AreaShape_MinorAxisLength',\n",
       " 'AreaShape_Orientation',\n",
       " 'AreaShape_Perimeter',\n",
       " 'AreaShape_Solidity',\n",
       " 'Intensity_IntegratedIntensityEdge_dapi',\n",
       " 'Intensity_IntegratedIntensity_dapi',\n",
       " 'Intensity_LowerQuartileIntensity_dapi',\n",
       " 'Intensity_MADIntensity_dapi',\n",
       " 'Intensity_MassDisplacement_dapi',\n",
       " 'Intensity_MaxIntensityEdge_dapi',\n",
       " 'Intensity_MaxIntensity_dapi',\n",
       " 'Intensity_MeanIntensityEdge_dapi',\n",
       " 'Intensity_MeanIntensity_dapi',\n",
       " 'Intensity_MedianIntensity_dapi',\n",
       " 'Intensity_MinIntensityEdge_dapi',\n",
       " 'Intensity_MinIntensity_dapi',\n",
       " 'Intensity_StdIntensityEdge_dapi',\n",
       " 'Intensity_StdIntensity_dapi',\n",
       " 'Intensity_UpperQuartileIntensity_dapi',\n",
       " 'in_object']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keep_cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''pull out parameters and target parameter'''\n",
    "df_test = df_200217[keep_cats[:-1]]\n",
    "#x_test = df_test.drop('in_object', axis = 1)\n",
    "x_test = sc_X.transform(df_test)\n",
    "\n",
    "\n",
    "'''make predictions based on the trained data'''\n",
    "y_pred = clf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_200217_test = df_200217.copy()\n",
    "df_200217_test['in_object'] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_200217_test = df_200217_test.loc[df_200217_test.in_object == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48328778642001263"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_200217_test) / len(df_200217)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With that all working well I am just going to run through the same training but with all of the data instead of just the training dta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None, var_smoothing=1e-09)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''split the data into train and target sets. In this caes x will be the train data and y will be the target parameter'''\n",
    "df = df_whole[keep_cats] # make a copy of original df. I am going to alter this a little bit in order to pull out train and test cats\n",
    "\n",
    "target_cat_name = 'in_object'\n",
    "\n",
    "'''remove unwanted categories'''\n",
    "#unwanted_cat = ['Unnamed: 0']#, 'ImageNumber', 'ObjectNumber']\n",
    "#df.drop(unwanted_cat, axis = 1, inplace = True)\n",
    "\n",
    "'''define train and target data'''\n",
    "x_train = df.drop('in_object', axis = 1)\n",
    "y_train = df[target_cat_name]\n",
    "\n",
    "#'''split data into train and test sets'''\n",
    "#x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = .25, random_state = 42)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''this is a feature scaling step. This will standardize all of the data in order to pull everything into the same range'''\n",
    "\n",
    "sc_X = StandardScaler()\n",
    "x_train = sc_X.fit_transform(x_train)\n",
    "#x_test = sc_X.transform(x_test)\n",
    "\n",
    "'''set up gaussian naive bayes model and fit it to the data'''\n",
    "model = GaussianNB()\n",
    "model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I need to generate the data frames that I would like to use for this analysis first. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''change to directory with annotated .csv files'''\n",
    "os.chdir(r'C:\\Users\\LegantLab\\Documents\\git\\tad\\Legant_lab\\200428_adipogenesis_timecourses_revisited\\190603_3t3l1_timecourse\\8kb_run\\annotation_csvs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['df_in_img1814.csv',\n",
       " 'df_in_img1854.csv',\n",
       " 'df_out_img1814.csv',\n",
       " 'df_out_img1854.csv']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''generate one big datafrme from the multiple annotated .csv files'''\n",
    "df_in_file_lst = ['df_in_img1814.csv',\n",
    " 'df_in_img1854.csv']\n",
    "df_out_file_lst = [ 'df_out_img1814.csv',\n",
    " 'df_out_img1854.csv']\n",
    "\n",
    "'''lists to be populated with dfs and concated a little later'''\n",
    "df_lst = []\n",
    "\n",
    "'''iterate over the .csv files and pull together one big dataframe'''\n",
    "\n",
    "for i in range(0, len(df_in_file_lst)):\n",
    "    df_in_ = pd.read_csv(df_in_file_lst[i])\n",
    "    df_in_['in_object'] = np.ones(len(df_in_))\n",
    "    df_lst.append(df_in_)\n",
    "for i in range(0, len(df_out_file_lst)):\n",
    "    df_out_ = pd.read_csv(df_out_file_lst[i])\n",
    "    df_out_['in_object'] = np.zeros(len(df_out_))\n",
    "    df_lst.append(df_out_)\n",
    "df_test = pd.concat(df_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''make list of features to be used in classification'''\n",
    "#raining_cats = list(df) #list of all of the features in the training df\n",
    "training_cats = keep_cats[:-1]#get rid of a couple of unwanted categories this is the unwanted: 1 as well as the in_count features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AreaShape_Area',\n",
       " 'AreaShape_Center_X',\n",
       " 'AreaShape_Center_Y',\n",
       " 'AreaShape_Center_Z',\n",
       " 'AreaShape_Compactness',\n",
       " 'AreaShape_Eccentricity',\n",
       " 'AreaShape_EulerNumber',\n",
       " 'AreaShape_Extent',\n",
       " 'AreaShape_FormFactor',\n",
       " 'AreaShape_MajorAxisLength',\n",
       " 'AreaShape_MaxFeretDiameter',\n",
       " 'AreaShape_MaximumRadius',\n",
       " 'AreaShape_MeanRadius',\n",
       " 'AreaShape_MedianRadius',\n",
       " 'AreaShape_MinFeretDiameter',\n",
       " 'AreaShape_MinorAxisLength',\n",
       " 'AreaShape_Orientation',\n",
       " 'AreaShape_Perimeter',\n",
       " 'AreaShape_Solidity',\n",
       " 'Intensity_IntegratedIntensityEdge_dapi',\n",
       " 'Intensity_IntegratedIntensity_dapi',\n",
       " 'Intensity_LowerQuartileIntensity_dapi',\n",
       " 'Intensity_MADIntensity_dapi',\n",
       " 'Intensity_MaxIntensityEdge_dapi',\n",
       " 'Intensity_MaxIntensity_dapi',\n",
       " 'Intensity_MeanIntensityEdge_dapi',\n",
       " 'Intensity_MeanIntensity_dapi',\n",
       " 'Intensity_MedianIntensity_dapi',\n",
       " 'Intensity_MinIntensityEdge_dapi',\n",
       " 'Intensity_MinIntensity_dapi',\n",
       " 'Intensity_StdIntensityEdge_dapi',\n",
       " 'Intensity_StdIntensity_dapi',\n",
       " 'Intensity_UpperQuartileIntensity_dapi',\n",
       " 'Location_CenterMassIntensity_X_dapi',\n",
       " 'Location_CenterMassIntensity_Y_dapi',\n",
       " 'Location_CenterMassIntensity_Z_dapi',\n",
       " 'Location_Center_X',\n",
       " 'Location_Center_Y',\n",
       " 'Location_Center_Z',\n",
       " 'Location_MaxIntensity_X_dapi',\n",
       " 'Location_MaxIntensity_Y_dapi',\n",
       " 'Location_MaxIntensity_Z_dapi']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''pull the categories out of the df that are used for classifictation'''\n",
    "x_df_test = df_test[training_cats]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''this is a feature scaling step. This will standardize all of the data in order to pull everything into the same range'''\n",
    "sc_X = StandardScaler()\n",
    "x_df_test = sc_X.fit_transform(x_df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['in_object_predict'] = model.predict(x_df_test) # add predictions to the df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(df_test.in_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(df_test.in_object_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7808219178082192"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.array(df_test.in_object == df_test.in_object_predict)) / len(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tp = len(df_test.loc[(df_test.in_object == 1) & (df_test.in_object_predict == 1)])\n",
    "tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = len(df_test.loc[(df_test.in_object == 0) & (df_test.in_object_predict == 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6590909090909091"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tp / (tp + fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''this is script that will read in the orignal dataframe. It takes in the orignal .csv file that is the output \n",
    "of CellProfiler. It returns two data frames with the time parameter added to each df. It splits the one large \n",
    "data frame according to whether or not the data is from the first set of images or the second set of images.\n",
    "After it adds the appropriate time parameter to each to each df'''\n",
    "\n",
    "'''change to directory with .csv flie'''\n",
    "os.chdir(r'C:\\Users\\timdaugird\\Google Drive\\legant_lab\\_projects\\200217_adipogen_zstacks\\cell_profiler_output\\dcts_output')\n",
    "\n",
    "df = pd.read_csv('MyExpt_nuclei.csv') #this the output of cell profiler\n",
    "\n",
    "'''split data sets'''\n",
    "df1_ = df.loc[df.ImageNumber <=120]\n",
    "df2_ = df.loc[df.ImageNumber > 120]\n",
    "\n",
    "'''list of times associated with each set of images'''\n",
    "times_lst = [-2, -1, 0, 2/24, 4/24, 6/24, 8/24, 1, 2, 3, 4, 6 ]\n",
    "times_lst = np.round(times_lst, 3)\n",
    "\n",
    "\n",
    "'''this is script to add the time parameter to each dataframe.'''\n",
    "\n",
    "'''start of with the first df'''\n",
    "img_numb_lst1 = list(range(np.min(df1_.ImageNumber), np.max(df1_.ImageNumber) + 2, 10)) #image numbers on lower and upper bounds for each time\n",
    "\n",
    "\n",
    "df_lst = [] #list to be populated\n",
    "\n",
    "for i in range(12):\n",
    "    df_ = df1_.loc[df1_.ImageNumber >= img_numb_lst1[i]] \n",
    "    df_ = df_.loc[df_.ImageNumber < img_numb_lst1[i+1]] #pull out specificed range of images\n",
    "    df_['time'] = np.zeros(len(df_)) # put in temporary array\n",
    "    df_.time = df_.time + times_lst[i] #add appropriate time\n",
    "    df_lst.append(df_) #append to running list\n",
    "    \n",
    "df1 = pd.concat(df_lst) #concat all df's into one bit one\n",
    "\n",
    "'''this is the same thing as the code immediatly above but for the second dataframe'''\n",
    "img_numb_lst2 = list(range(np.min(df2_.ImageNumber), np.max(df2_.ImageNumber) + 2, 10))\n",
    "df_lst = []\n",
    "\n",
    "for i in range(12):\n",
    "    df_ = df2_.loc[df2_.ImageNumber >= img_numb_lst2[i]]\n",
    "    df_ = df_.loc[df_.ImageNumber < img_numb_lst2[i+1]] #pull out specificed range of images\n",
    "    df_['time'] = np.zeros(len(df_)) # put in temporary array\n",
    "    df_.time = df_.time + times_lst[i]\n",
    "    df_lst.append(df_)\n",
    "    \n",
    "df2 = pd.concat(df_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''make list of features to be used in classification'''\n",
    "training_cats = list(df) #list of all of the features in the training df\n",
    "#training_cats = training_cats[1:-1] #get rid of a couple of unwanted categories this is the unwanted: 1 as well as the in_count features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''pull the categories out of the df that are used for classifictation'''\n",
    "x_train_df1 = df1[training_cats]\n",
    "x_train_df2 = df2[training_cats]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2808, 109)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(x_train_df1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(94, 109)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''this is a feature scaling step. This will standardize all of the data in order to pull everything into the same range'''\n",
    "sc_X = StandardScaler()\n",
    "x_train_df1 = sc_X.fit_transform(x_train_df1)\n",
    "x_train_df2 = sc_X.fit_transform(x_train_df2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''add the predictions to the data frame and keep only the predicted in entries'''\n",
    "\n",
    "df1['in_object'] = model.predict(x_train_df1) # add predictions to the df\n",
    "df1_nb_parsed = df1.loc[df1.in_object == 1] # keep only the predicted values\n",
    "\n",
    "df2['in_object'] = model.predict(x_train_df2) \n",
    "df2_nb_parsed = df1.loc[df1.in_object == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''create parsed data frames using original parameters'''\n",
    "df1_thresh_parsed = df1.loc[df1.AreaShape_Area > 20000]\n",
    "df1_thresh_parsed = df1_thresh_parsed[df1_thresh_parsed.AreaShape_Eccentricity >= .55]\n",
    "\n",
    "df2_thresh_parsed = df2.loc[df2.AreaShape_Area > 20000]\n",
    "df2_thresh_parsed = df2_thresh_parsed[df2_thresh_parsed.AreaShape_Eccentricity >= .55]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6315404317599708"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df2_nb_parsed) / len(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4921331869740212"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df2_thresh_parsed) / len(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1d30a3bc7c8>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''look at how distributions of areas are affected'''\n",
    "\n",
    "plt.subplot(131)\n",
    "sns.distplot(df2.AreaShape_Area)\n",
    "plt.subplot(132)\n",
    "sns.distplot(df2_thresh_parsed.AreaShape_Area)\n",
    "plt.subplot(133)\n",
    "sns.distplot(df2_nb_parsed.AreaShape_Area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1d30a9b2708>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sns.distplot(df2.AreaShape_Area)\n",
    "sns.distplot(df2_thresh_parsed.AreaShape_Area)\n",
    "sns.distplot(df2_nb_parsed.AreaShape_Area)\n",
    "plt.legend(['orig', 'threh', 'nb'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.lineplot(df1_thresh_parsed.time, df1_thresh_parsed.Intensity_IntegratedIntensity_cebp)\n",
    "ax = sns.lineplot(df1_nb_parsed.time, df1_nb_parsed.Intensity_IntegratedIntensity_cebp)\n",
    "\n",
    "plt.legend(['thresh_filter', 'nb_filter'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2733"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.lineplot(df1_nb_parsed.time, df1_nb_parsed.Intensity_IntegratedIntensity_cebp, color = 'm')\n",
    "ax = sns.lineplot(df2_nb_parsed.time, df2_nb_parsed.Intensity_IntegratedIntensity_cebp, color = 'y')\n",
    "\n",
    "plt.legend(['df1', 'df2'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I am thinking that the above looks pretty okay. I am actually quite happy with it thus far. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_nb_parsed.to_csv('df1_nb_parsed.csv')\n",
    "df2_nb_parsed.to_csv('df2_nb_parsed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''temporary lists to be populated with the number of objects'''\n",
    "df1_orig_ens = []\n",
    "df1_ens = []\n",
    "df2_orig_ens = []\n",
    "df2_ens = []\n",
    "\n",
    "'''iterate over the times and figure out how many entries there are for each df at a given time point.'''\n",
    "for i in np.unique(df1.time):\n",
    "    '''original df1'''\n",
    "    df_ = df1.loc[df1.time == i] # pull out single time \n",
    "    df1_orig_ens.append(len(df_)) #append list with number of objects at given time point\n",
    "    \n",
    "    '''parsed df1'''\n",
    "    df_ = df1_nb_parsed.loc[df1_nb_parsed.time == i]\n",
    "    df1_ens.append(len(df_))\n",
    "    \n",
    "    '''original df2'''\n",
    "    df_ = df2.loc[df2.time == i]\n",
    "    df2_orig_ens.append(len(df_))\n",
    "    \n",
    "    '''parsed df2'''\n",
    "    df_ = df2_nb_parsed.loc[df2_nb_parsed.time == i]\n",
    "    df2_ens.append(len(df_))\n",
    "    \n",
    "df_ens = pd.DataFrame({'time_point' : times_lst,\n",
    "                     'original_df1_N' : df1_orig_ens,\n",
    "                     'parsed_df1_N' : df1_ens,\n",
    "                     'original_df2_N' : df2_orig_ens,\n",
    "                     'parsed_df2_N' : df2_ens})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time_point</th>\n",
       "      <th>original_df1_N</th>\n",
       "      <th>parsed_df1_N</th>\n",
       "      <th>original_df2_N</th>\n",
       "      <th>parsed_df2_N</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-2.000</td>\n",
       "      <td>221</td>\n",
       "      <td>131</td>\n",
       "      <td>253</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.000</td>\n",
       "      <td>230</td>\n",
       "      <td>162</td>\n",
       "      <td>207</td>\n",
       "      <td>162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000</td>\n",
       "      <td>223</td>\n",
       "      <td>140</td>\n",
       "      <td>226</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.083</td>\n",
       "      <td>208</td>\n",
       "      <td>131</td>\n",
       "      <td>190</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.167</td>\n",
       "      <td>239</td>\n",
       "      <td>143</td>\n",
       "      <td>210</td>\n",
       "      <td>143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.250</td>\n",
       "      <td>190</td>\n",
       "      <td>134</td>\n",
       "      <td>236</td>\n",
       "      <td>134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.333</td>\n",
       "      <td>179</td>\n",
       "      <td>121</td>\n",
       "      <td>201</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.000</td>\n",
       "      <td>220</td>\n",
       "      <td>141</td>\n",
       "      <td>216</td>\n",
       "      <td>141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2.000</td>\n",
       "      <td>251</td>\n",
       "      <td>164</td>\n",
       "      <td>230</td>\n",
       "      <td>164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3.000</td>\n",
       "      <td>276</td>\n",
       "      <td>181</td>\n",
       "      <td>256</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4.000</td>\n",
       "      <td>255</td>\n",
       "      <td>172</td>\n",
       "      <td>241</td>\n",
       "      <td>172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>6.000</td>\n",
       "      <td>316</td>\n",
       "      <td>106</td>\n",
       "      <td>267</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    time_point  original_df1_N  parsed_df1_N  original_df2_N  parsed_df2_N\n",
       "0       -2.000             221           131             253           131\n",
       "1       -1.000             230           162             207           162\n",
       "2        0.000             223           140             226           140\n",
       "3        0.083             208           131             190           131\n",
       "4        0.167             239           143             210           143\n",
       "5        0.250             190           134             236           134\n",
       "6        0.333             179           121             201           121\n",
       "7        1.000             220           141             216           141\n",
       "8        2.000             251           164             230           164\n",
       "9        3.000             276           181             256           181\n",
       "10       4.000             255           172             241           172\n",
       "11       6.000             316           106             267           106"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1d30f6be248>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sns.violinplot(df2_nb_parsed.time, df2_nb_parsed.AreaShape_Area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\timdaugird\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "'''make one big df from the df2'''\n",
    "df2_thresh_parsed['df_id'] = np.zeros(len(df2_thresh_parsed))\n",
    "df2_nb_parsed['df_id'] = np.ones(len(df2_nb_parsed))\n",
    "\n",
    "df2_big = pd.concat([df2_thresh_parsed, df2_nb_parsed])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1d3196562c8>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sns.boxplot(df2_big.time, df2_big.AreaShape_Area, hue = df2_big.df_id, fliersize = 0)\n",
    "#plt.legend(['thresh', 'nb'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1d31c19c748>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sns.lineplot(df2_thresh_parsed.time, df2_thresh_parsed.AreaShape_Area)\n",
    "sns.lineplot(df2_nb_parsed.time, df2_nb_parsed.AreaShape_Area)\n",
    "plt.legend(['thresh', 'nb'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
